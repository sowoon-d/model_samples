{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\h2009134\\jupyterlab\\huggingface_konlpy-master.zip\n",
      "Requirement already satisfied: konlpy>=0.5.2 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from huggingface-konlpy==0.1.0) (0.5.2)\n",
      "Requirement already satisfied: tokenizers==0.8.1 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from huggingface-konlpy==0.1.0) (0.8.1)\n",
      "Collecting transformers==3.0.2\n",
      "  Using cached transformers-3.0.2-py3-none-any.whl (769 kB)\n",
      "Requirement already satisfied: tqdm>=4.46.0 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from huggingface-konlpy==0.1.0) (4.50.2)\n",
      "Collecting wandb\n",
      "  Using cached wandb-0.10.8-py2.py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from konlpy>=0.5.2->huggingface-konlpy==0.1.0) (1.0.2)\n",
      "Requirement already satisfied: tweepy>=3.7.0 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from konlpy>=0.5.2->huggingface-konlpy==0.1.0) (3.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from konlpy>=0.5.2->huggingface-konlpy==0.1.0) (0.4.3)\n",
      "Requirement already satisfied: beautifulsoup4==4.6.0 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from konlpy>=0.5.2->huggingface-konlpy==0.1.0) (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from konlpy>=0.5.2->huggingface-konlpy==0.1.0) (1.19.1)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from konlpy>=0.5.2->huggingface-konlpy==0.1.0) (4.5.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers==3.0.2->huggingface-konlpy==0.1.0) (20.4)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers==3.0.2->huggingface-konlpy==0.1.0) (0.1.94)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers==3.0.2->huggingface-konlpy==0.1.0) (0.0.43)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers==3.0.2->huggingface-konlpy==0.1.0) (2020.10.15)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers==3.0.2->huggingface-konlpy==0.1.0) (0.7)\n",
      "Requirement already satisfied: requests in c:\\users\\h2009134\\appdata\\roaming\\python\\python36\\site-packages (from transformers==3.0.2->huggingface-konlpy==0.1.0) (2.24.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers==3.0.2->huggingface-konlpy==0.1.0) (3.0.12)\n",
      "Requirement already satisfied: six>=1.13.0 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from wandb->huggingface-konlpy==0.1.0) (1.15.0)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Using cached GitPython-3.1.11-py3-none-any.whl (159 kB)\n",
      "Processing c:\\users\\h2009134\\appdata\\local\\pip\\cache\\wheels\\59\\9a\\1d\\3f1afbbb5122d0410547bf9eb50955f4a7a98e53a6d8b99bd1\\promise-2.3-py3-none-any.whl\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Using cached shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
      "Processing c:\\users\\h2009134\\appdata\\local\\pip\\cache\\wheels\\44\\3a\\ab\\102386d84fe551b6cedb628ed1e74c5f5be76af8b909aeda09\\subprocess32-3.5.4-py3-none-any.whl\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from wandb->huggingface-konlpy==0.1.0) (5.7.2)\n",
      "Collecting configparser>=3.8.1\n",
      "  Using cached configparser-5.0.1-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from wandb->huggingface-konlpy==0.1.0) (3.13.0)\n",
      "Requirement already satisfied: Click>=7.0 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from wandb->huggingface-konlpy==0.1.0) (7.1.2)\n",
      "Collecting sentry-sdk>=0.4.0\n",
      "  Using cached sentry_sdk-0.19.1-py2.py3-none-any.whl (120 kB)\n",
      "Requirement already satisfied: watchdog>=0.8.3 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from wandb->huggingface-konlpy==0.1.0) (0.10.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script transformers-cli.exe is installed in 'C:\\Users\\H2009134\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts wandb.exe and wb.exe are installed in 'C:\\Users\\H2009134\\AppData\\Roaming\\Python\\Python36\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "transformers 3.0.2 requires tokenizers==0.8.1.rc1, but you'll have tokenizers 0.8.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyYAML in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from wandb->huggingface-konlpy==0.1.0) (5.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from wandb->huggingface-konlpy==0.1.0) (2.8.1)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from JPype1>=0.7.0->konlpy>=0.5.2->huggingface-konlpy==0.1.0) (3.7.4.3)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from tweepy>=3.7.0->konlpy>=0.5.2->huggingface-konlpy==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from packaging->transformers==3.0.2->huggingface-konlpy==0.1.0) (2.4.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from sacremoses->transformers==3.0.2->huggingface-konlpy==0.1.0) (0.17.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->transformers==3.0.2->huggingface-konlpy==0.1.0) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->transformers==3.0.2->huggingface-konlpy==0.1.0) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->transformers==3.0.2->huggingface-konlpy==0.1.0) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->transformers==3.0.2->huggingface-konlpy==0.1.0) (3.0.4)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.5-py3-none-any.whl (63 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from protobuf>=3.12.0->wandb->huggingface-konlpy==0.1.0) (50.3.0.post20201006)\n",
      "Requirement already satisfied: pathtools>=0.1.1 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from watchdog>=0.8.3->wandb->huggingface-konlpy==0.1.0) (0.1.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy>=0.5.2->huggingface-konlpy==0.1.0) (3.1.0)\n",
      "Collecting smmap<4,>=3.0.1\n",
      "  Using cached smmap-3.0.4-py2.py3-none-any.whl (25 kB)\n",
      "Building wheels for collected packages: huggingface-konlpy\n",
      "  Building wheel for huggingface-konlpy (setup.py): started\n",
      "  Building wheel for huggingface-konlpy (setup.py): finished with status 'done'\n",
      "  Created wheel for huggingface-konlpy: filename=huggingface_konlpy-0.1.0-py3-none-any.whl size=9375 sha256=e2b5a4b95cf8e500776a04e4a2acea221a295ecc951911f7009d1e8b4ffdd6e6\n",
      "  Stored in directory: c:\\users\\h2009134\\appdata\\local\\pip\\cache\\wheels\\d7\\1a\\9a\\59890b5c98531c88c193fccdb78445dcc367945bed874d32e7\n",
      "Successfully built huggingface-konlpy\n",
      "Installing collected packages: transformers, smmap, gitdb, GitPython, promise, shortuuid, subprocess32, docker-pycreds, configparser, sentry-sdk, wandb, huggingface-konlpy\n",
      "Successfully installed GitPython-3.1.11 configparser-5.0.1 docker-pycreds-0.4.0 gitdb-4.0.5 huggingface-konlpy-0.1.0 promise-2.3 sentry-sdk-0.19.1 shortuuid-1.0.1 smmap-3.0.4 subprocess32-3.5.4 transformers-3.0.2 wandb-0.10.8\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_konlpy-master.zip --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (0.9.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-3.4.0-py3-none-any.whl (1.3 MB)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers) (4.50.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: packaging in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers) (2020.10.15)\n",
      "Collecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading dataclasses-0.7-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: protobuf in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers) (3.13.0)\n",
      "Requirement already satisfied: requests in c:\\users\\h2009134\\appdata\\roaming\\python\\python36\\site-packages (from transformers) (2.24.0)\n",
      "Collecting tokenizers==0.9.2\n",
      "  Downloading tokenizers-0.9.2-cp36-cp36m-win_amd64.whl (1.9 MB)\n",
      "Collecting sentencepiece!=0.1.92\n",
      "  Downloading sentencepiece-0.1.94-cp36-cp36m-win_amd64.whl (1.2 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from transformers) (1.19.1)\n",
      "Requirement already satisfied: six in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: setuptools in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from protobuf->transformers) (50.3.0.post20201006)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\h2009134\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893262 sha256=cdf9a9594ce70c672cc609aec91ca64c0cb66485320f2f2de43edc9da665d8fc\n",
      "  Stored in directory: c:\\users\\h2009134\\appdata\\local\\pip\\cache\\wheels\\49\\25\\98\\cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, dataclasses, tokenizers, sentencepiece, transformers\n",
      "Successfully installed dataclasses-0.7 sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.9.2 transformers-3.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 18s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%time AIMS = pd.read_excel('data/HKMC_MEL_MST_LIST.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137355"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aims = AIMS[['CAU_DESC','REQ_DESC','RSL_DESC']].drop_duplicates()\n",
    "len(df_aims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df_aims['CAU_DESC'] + '. ' + df_aims['REQ_DESC'] + '. ' + df_aims['RSL_DESC'] + '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('result/test.txt',header=None,index=False,sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers_bert_tokenizer = BertTokenizer(vocab_file = 'result/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_wordpiece_tokenizer = BertWordPieceTokenizer()\n",
    "bert_wordpiece_tokenizer.train(\n",
    "    files = ['result/test.txt'],\n",
    "    vocab_size = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers  : ['신', '##종', '코', '##로', '##나', '##바', '##이', '##러', '##스', '감', '##여', '##ᆷ', '##증', '(', '코', '##로', '##나', '##19', ')', '사', '##태', '##가', '시', '##ᆷ', '##각', '##합', '##니다']\n",
      "transformers: ['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', ')', '[UNK]', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "sent_ko = '신종 코로나바이러스 감염증(코로나19) 사태가 심각합니다'\n",
    "print(f'tokenizers  : {bert_wordpiece_tokenizer.encode(sent_ko).tokens}')\n",
    "print(f'transformers: {transformers_bert_tokenizer.tokenize(sent_ko)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers  : ['1', '.', '(', '설계', ')', '와이어링', '인출', '##구', '##측', '프로텍터', '최대', '##한', '0', '##l', '##측으로', '이동', '##하여', '손간섭', '개선', '2', '.', '(', '의장', '##5', '##1부', ')', '해당', '커넥터', '엔진', '##서', '##브', '##반', '선', '##작업', '가능', '##여부', '확인', '##요망']\n",
      "transformers: ['[UNK]', '.', '[UNK]', '[UNK]', ')', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '.', '[UNK]', '[UNK]', ')', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "sent_ko = '1. (설계)와이어링 인출구측 프로텍터 최대한 0L측으로 이동하여 손간섭 개선 2. (의장51부) 해당 커넥터 엔진서브반 선작업 가능여부 확인요망'\n",
    "print(f'tokenizers  : {bert_wordpiece_tokenizer.encode(sent_ko).tokens}')\n",
    "print(f'transformers: {transformers_bert_tokenizer.tokenize(sent_ko)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_konlpy.tokenizers_konlpy import KoNLPyBertWordPieceTrainer\n",
    "from huggingface_konlpy.transformers_konlpy import KoNLPyBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eunjeon import Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initialize alphabet 1/1: 100%|██████████| 1000584/1000584 [00:03<00:00, 324495.85it/s]\n",
      "Train vocab 1/1:   0%|          | 0/1000584 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pos() got an unexpected keyword argument 'join'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-67402280a9a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     Mecab(), use_tag=True)\n\u001b[0;32m      3\u001b[0m mecab_wordpiece_usetag_trainer.train(\n\u001b[1;32m----> 4\u001b[1;33m     files = ['result/test.txt'])\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\huggingface_konlpy\\tokenizers_konlpy\\tokenizers.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, files, vocab_size, min_frequency, limit_alphabet, initial_alphabet, special_tokens, show_progress)\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0malphabets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitialize_alphabet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit_alphabet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_alphabet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecial_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_frequency\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malphabets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\huggingface_konlpy\\tokenizers_konlpy\\tokenizers.py\u001b[0m in \u001b[0;36mtrain_vocab\u001b[1;34m(files, vocab_size, min_frequency, show_progress, alphabets, tokenize)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[0miterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mcounter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msub\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m     \u001b[0mfrequent_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     frequent_vocab = [vocab for vocab in frequent_vocab\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    533\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'expected at most 1 arguments, got %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 535\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    620\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# fast path when counter is empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 622\u001b[1;33m                 \u001b[0m_count_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    623\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\huggingface_konlpy\\tokenizers_konlpy\\tokenizers.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[0miterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mcounter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msub\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m     \u001b[0mfrequent_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     frequent_vocab = [vocab for vocab in frequent_vocab\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\huggingface_konlpy\\tokenizers_konlpy\\pretokenizers.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, sent)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0meojeol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0msplit_tokens\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0m_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meojeol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msplit_tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\huggingface_konlpy\\tokenizers_konlpy\\pretokenizers.py\u001b[0m in \u001b[0;36m_tokenize\u001b[1;34m(eojeol)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meojeol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0msplit_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meojeol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_tokens\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0msplit_tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\huggingface_konlpy\\tokenizers_konlpy\\pretokenizers.py\u001b[0m in \u001b[0;36mpretokenize\u001b[1;34m(eojeol)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0muse_tag\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mpretokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meojeol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkonlpy_tagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meojeol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mpretokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meojeol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: pos() got an unexpected keyword argument 'join'"
     ]
    }
   ],
   "source": [
    "mecab_wordpiece_usetag_trainer = KoNLPyBertWordPieceTrainer(\n",
    "    Mecab(), use_tag=True)\n",
    "mecab_wordpiece_usetag_trainer.train(\n",
    "    files = ['result/test.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Initialize alphabet 1/1:   0%|          | 0/70964 [00:00<?, ?it/s]\u001b[A\n",
      "Initialize alphabet 1/1:   9%|▉         | 6281/70964 [00:00<00:01, 62623.43it/s]\u001b[A\n",
      "Initialize alphabet 1/1:  19%|█▊        | 13266/70964 [00:00<00:00, 64304.21it/s]\u001b[A\n",
      "Initialize alphabet 1/1:  27%|██▋       | 19102/70964 [00:00<00:00, 62382.95it/s]\u001b[A\n",
      "Initialize alphabet 1/1:  35%|███▌      | 24919/70964 [00:00<00:00, 60899.96it/s]\u001b[A\n",
      "Initialize alphabet 1/1:  46%|████▋     | 32849/70964 [00:00<00:00, 63134.46it/s]\u001b[A\n",
      "Initialize alphabet 1/1:  56%|█████▋    | 40028/70964 [00:00<00:00, 65301.80it/s]\u001b[A\n",
      "Initialize alphabet 1/1:  68%|██████▊   | 48009/70964 [00:00<00:00, 66382.78it/s]\u001b[A\n",
      "Initialize alphabet 1/1:  78%|███████▊  | 55148/70964 [00:00<00:00, 67596.68it/s]\u001b[A\n",
      "Initialize alphabet 1/1:  87%|████████▋ | 61577/70964 [00:00<00:00, 66560.60it/s]\u001b[A\n",
      "Initialize alphabet 1/1: 100%|██████████| 70964/70964 [00:01<00:00, 65305.90it/s]\u001b[A\n",
      "\n",
      "Train vocab 1/1:   0%|          | 0/70964 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pos() got an unexpected keyword argument 'join'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-46d0381a3670>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m mecab_wordpiece_usetag_trainer.train(\n\u001b[1;32m----> 2\u001b[1;33m     files = ['huggingface_konlpy-master/data/2020-07-29_covid_news_sents.txt'])\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\huggingface_konlpy\\tokenizers_konlpy\\tokenizers.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, files, vocab_size, min_frequency, limit_alphabet, initial_alphabet, special_tokens, show_progress)\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0malphabets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitialize_alphabet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit_alphabet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_alphabet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecial_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_frequency\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malphabets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\huggingface_konlpy\\tokenizers_konlpy\\tokenizers.py\u001b[0m in \u001b[0;36mtrain_vocab\u001b[1;34m(files, vocab_size, min_frequency, show_progress, alphabets, tokenize)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[0miterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mcounter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msub\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m     \u001b[0mfrequent_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     frequent_vocab = [vocab for vocab in frequent_vocab\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    533\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'expected at most 1 arguments, got %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 535\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__missing__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    620\u001b[0m                     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# fast path when counter is empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 622\u001b[1;33m                 \u001b[0m_count_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    623\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\huggingface_konlpy\\tokenizers_konlpy\\tokenizers.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[0miterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mcounter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msub\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m     \u001b[0mfrequent_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     frequent_vocab = [vocab for vocab in frequent_vocab\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\huggingface_konlpy\\tokenizers_konlpy\\pretokenizers.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, sent)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0meojeol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0msplit_tokens\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0m_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meojeol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msplit_tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\huggingface_konlpy\\tokenizers_konlpy\\pretokenizers.py\u001b[0m in \u001b[0;36m_tokenize\u001b[1;34m(eojeol)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meojeol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0msplit_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meojeol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_tokens\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0msplit_tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\huggingface_konlpy\\tokenizers_konlpy\\pretokenizers.py\u001b[0m in \u001b[0;36mpretokenize\u001b[1;34m(eojeol)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0muse_tag\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mpretokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meojeol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkonlpy_tagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meojeol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mpretokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meojeol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: pos() got an unexpected keyword argument 'join'"
     ]
    }
   ],
   "source": [
    "mecab_wordpiece_usetag_trainer.train(\n",
    "    files = ['huggingface_konlpy-master/data/2020-07-29_covid_news_sents.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--limit_alphabet'], dest='limit_alphabet', nargs=None, const=None, default=6000, type=<class 'int'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--corpus_file\", type=str)\n",
    "parser.add_argument(\"--vocab_size\", type=int, default=32000)\n",
    "parser.add_argument(\"--limit_alphabet\", type=int, default=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--corpus_file CORPUS_FILE]\n",
      "                             [--vocab_size VOCAB_SIZE]\n",
      "                             [--limit_alphabet LIMIT_ALPHABET]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\H2009134\\AppData\\Roaming\\jupyter\\runtime\\kernel-96790888-c07d-4cad-a8cc-9676bc8868ce.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\H2009134\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args()\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    vocab_file=None,\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=False, # Must be False if cased model\n",
    "    lowercase=False,\n",
    "    wordpieces_prefix=\"##\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tokenizer==0.7.0 (from versions: 0.1.1, 0.1.2, 0.1.3, 0.1.10, 0.1.11, 0.1.12, 0.1.14, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.0.5, 1.0.6, 1.0.7, 1.0.8, 1.0.9, 1.1.0, 1.1.1, 1.1.2, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.3.0, 1.4.0, 1.4.1, 2.0.0, 2.0.1, 2.0.2, 2.0.3, 2.0.4, 2.0.5, 2.0.6, 2.0.7, 2.1.0, 2.2.0, 2.3.0, 2.3.1, 2.4.0)\n",
      "ERROR: No matching distribution found for tokenizer==0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizer==0.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "corpus_file   = 'result/test.txt'\n",
    "vocab_size    = 32000\n",
    "limit_alphabet= 6000\n",
    "output_path   = 'hugging_%d'%(vocab_size)\n",
    "min_frequency = 5\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    vocab_file=None,\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=False,  # Must be False if cased model\n",
    "    lowercase=False,\n",
    "    wordpieces_prefix=\"##\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-fe4712b66fb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m                     \u001b[1;34m'[unused70]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused71]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused72]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused73]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused74]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused75]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused76]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused77]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused78]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused79]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                     \u001b[1;34m'[unused80]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused81]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused82]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused83]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused84]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused85]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused86]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused87]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused88]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused89]'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                     \u001b[1;34m'[unused90]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused91]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused92]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused93]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused94]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused95]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused96]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused97]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused98]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[unused99]'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                    ]  # 스페셜 토큰\n\u001b[0;32m     24\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: train() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "vocab_size    = 32000\n",
    "min_frequency = 3\n",
    "limit_alphabet= 6000\n",
    "\n",
    "trainer = BertWordPieceTokenizer.train(\n",
    "    files=['result/test.txt'],\n",
    "    vocab_size=vocab_size,  # 사전의 최대 크기, 32000\n",
    "    min_frequency=min_frequency,  # 단어의 최소 발생 빈도, 3\n",
    "    show_progress=True,\n",
    "    limit_alphabet=limit_alphabet,\n",
    "    special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]',\n",
    "                    '[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]',\n",
    "                    '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]',\n",
    "                    '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]',\n",
    "                    '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]',\n",
    "                    '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]',\n",
    "                    '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]', '[unused49]',\n",
    "                    '[unused50]', '[unused51]', '[unused52]', '[unused53]', '[unused54]', '[unused55]', '[unused56]', '[unused57]', '[unused58]', '[unused59]',\n",
    "                    '[unused60]', '[unused61]', '[unused62]', '[unused63]', '[unused64]', '[unused65]', '[unused66]', '[unused67]', '[unused68]', '[unused69]',\n",
    "                    '[unused70]', '[unused71]', '[unused72]', '[unused73]', '[unused74]', '[unused75]', '[unused76]', '[unused77]', '[unused78]', '[unused79]',\n",
    "                    '[unused80]', '[unused81]', '[unused82]', '[unused83]', '[unused84]', '[unused85]', '[unused86]', '[unused87]', '[unused88]', '[unused89]',\n",
    "                    '[unused90]', '[unused91]', '[unused92]', '[unused93]', '[unused94]', '[unused95]', '[unused96]', '[unused97]', '[unused98]', '[unused99]'\n",
    "                   ]  # 스페셜 토큰\n",
    ")\n",
    "# Save the files\n",
    "huggingface_tokenizer.save_model('.', 'huggingface_tokenizer_kor_32000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Error while initializing WordPiece",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-2d610ade2b5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mstrip_accents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# Cased 모델 시 False\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mwordpieces_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"##\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tokenizers\\implementations\\bert_wordpiece.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vocab_file, unk_token, sep_token, cls_token, pad_token, mask_token, clean_text, handle_chinese_chars, strip_accents, lowercase, wordpieces_prefix)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvocab_file\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWordPiece\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munk_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munk_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWordPiece\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munk_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munk_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Error while initializing WordPiece"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    vocab_file=output_path+'-vocab.txt',\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=False, # Cased 모델 시 False\n",
    "    lowercase=False,\n",
    "    wordpieces_prefix=\"##\"\n",
    ")\n",
    "\n",
    "output = tokenizer.encode(\"나는 오늘 아침밥을 먹었다.\")\n",
    "print('idx   : %s'%output.ids)\n",
    "print('tokens: %s'%output.tokens)\n",
    "print('offset: %s'%output.offsets)\n",
    "\n",
    "output = tokenizer.encode(\"교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\")\n",
    "print('idx   : %s'%output.ids)\n",
    "print('tokens: %s'%output.tokens)\n",
    "print('offset: %s'%output.offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--corpus_file CORPUS_FILE]\n",
      "                             [--vocab_size VOCAB_SIZE]\n",
      "                             [--limit_alphabet LIMIT_ALPHABET]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\H2009134\\AppData\\Roaming\\jupyter\\runtime\\kernel-2ae7bd63-435b-47c1-9ccf-64e8b9d8b7d4.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\H2009134\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--corpus_file\", type=str)\n",
    "parser.add_argument(\"--vocab_size\", type=int, default=32000)\n",
    "parser.add_argument(\"--limit_alphabet\", type=int, default=6000)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    vocab_file=None,\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=False, # Must be False if cased model\n",
    "    lowercase=False,\n",
    "    wordpieces_prefix=\"##\"\n",
    ")\n",
    "\n",
    "tokenizer.train(\n",
    "    files=[args.corpus_file],\n",
    "    limit_alphabet=args.limit_alphabet,\n",
    "    vocab_size=args.vocab_size\n",
    ")\n",
    "\n",
    "tokenizer.save(\"./\", \"ch-{}-wpm-{}\".format(args.limit_alphabet, args.vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Model name 'bert-base-uncased' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). We assumed 'bert-base-uncased' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-0a4553e5cdd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bert-base-uncased\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"I have a new GPU!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m         \"\"\"\n\u001b[1;32m-> 1140\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_from_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1244\u001b[0m                     \u001b[1;34m\", \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms3_models\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1245\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1246\u001b[1;33m                     \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_files_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1247\u001b[0m                 )\n\u001b[0;32m   1248\u001b[0m             )\n",
      "\u001b[1;31mOSError\u001b[0m: Model name 'bert-base-uncased' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). We assumed 'bert-base-uncased' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url."
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.tokenize(\"I have a new GPU!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (ByteLevelBPETokenizer,\n",
    "                            CharBPETokenizer,\n",
    "                            SentencePieceBPETokenizer,\n",
    "                            BertWordPieceTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train([\"result/test.txt\"], vocab_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.encode(\"1. (설계)와이어링 인출구측 프로텍터 최대한 0L측으로 이동하여 손간섭 개선 2. (의장51부) 해당 커넥터 엔진서브반 선작업 가능여부 확인요망\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 13, 335, 386, 8, 818, 7518, 455, 1064, 3818, 1168, 15739, 3310, 1709, 551, 508, 13, 335, 1321, 2349, 290, 8, 1046, 533, 15038, 1946, 6039, 3220, 8919] ['1', '.', 'Ġ(', 'ìĦ¤ê³Ħ', ')', 'ìĻĢìĿ´ìĸ´ë§ģ', 'ĠìĿ¸ì¶ľêµ¬', 'ì¸¡', 'ĠíĶĦë¡ľíħįíĦ°', 'ĠìµľëĮĢíķľ', 'Ġ0', 'Lì¸¡ìľ¼ë¡ľ', 'ĠìĿ´ëıĻíķĺìĹ¬', 'ĠìĨĲê°ĦìĦŃ', 'Ġê°ľìĦł', 'Ġ2', '.', 'Ġ(', 'ìĿĺìŀ¥', '51', 'ë¶Ģ', ')', 'Ġíķ´ëĭ¹', 'Ġì»¤ëĦ¥íĦ°', 'ĠìĹĶì§ĦìĦľë¸Į', 'ë°ĺ', 'ĠìĦłìŀĳìĹħ', 'Ġê°ĢëĬ¥ìĹ¬ë¶Ģ', 'ĠíĻķìĿ¸ìļĶë§Ŀ'] [(0, 1), (1, 2), (2, 4), (4, 6), (6, 7), (7, 11), (11, 15), (15, 16), (16, 21), (21, 25), (25, 27), (27, 31), (31, 36), (36, 40), (40, 43), (43, 45), (45, 46), (46, 48), (48, 50), (50, 52), (52, 53), (53, 54), (54, 57), (57, 61), (61, 66), (66, 67), (67, 71), (71, 76), (76, 81)]\n"
     ]
    }
   ],
   "source": [
    "print(output.ids, output.tokens, output.offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
